---
layout: post
title:  "On Pruning Large Language Model's"
date:   2023-04-2 00:13:24 +0530
categories: blog ml
comments: false
---

This blogpost describe the compressing of BERT, in the context of the Lottery Ticket Hypothesis. Through emperical evidence obtained by fine tuning on several tasks, it is found that *30-40%* of the parameters in the BERT model can be discarded. 

<!--more-->

{% pdf "/pdfs/pruning_DNN.pdf" %}

