<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-03-14T23:54:26+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Amrita H Nair</title><subtitle>Amrita Nair's blog</subtitle><entry><title type="html">A glimpse into Open Information Extraction with Large Language Models</title><link href="http://localhost:4000/blog/ml/2023/06/22/lct-annual-meeting-poster.html" rel="alternate" type="text/html" title="A glimpse into Open Information Extraction with Large Language Models" /><published>2023-06-22T12:17:24+02:00</published><updated>2023-06-22T12:17:24+02:00</updated><id>http://localhost:4000/blog/ml/2023/06/22/lct-annual-meeting-poster</id><content type="html" xml:base="http://localhost:4000/blog/ml/2023/06/22/lct-annual-meeting-poster.html">&lt;p&gt;This poster was presented at the Annual LCT Meeting, University of Malta, June 2023 and is based on the work by Wang et al. Large Language Models(LLM’s) store linguistic and relational information. The knowledge gained by LLM’s can be exploited and extracted to perform partially unsupervised open information extraction. Extracting n-ary representations from text in an unsupervised manner helps in many tasks, such as building knowledge bases and knowledge graph construction.&lt;/p&gt;

&lt;!--more--&gt;

&lt;style&gt; .pdf-embed-wrap-dee20d12-d409-41d3-9075-6a820e31caab { display: flex; flex-direction: column; width: 100%; height: 650px; } .pdf-embed-container-dee20d12-d409-41d3-9075-6a820e31caab { height: 100%; } .pdf-link-dee20d12-d409-41d3-9075-6a820e31caab { background-color: white; text-align: center; border-style: solid; } .pdf-embed-container-dee20d12-d409-41d3-9075-6a820e31caab iframe { width: 100%; height: 100%; } &lt;/style&gt;
&lt;div class=&quot;pdf-embed-wrap-dee20d12-d409-41d3-9075-6a820e31caab&quot;&gt; &lt;div class=&quot;pdf-link-dee20d12-d409-41d3-9075-6a820e31caab&quot;&gt; &lt;a href=&quot;/pdfs/LCT_Annual_Meeting_2023.pdf&quot; target=&quot;_blank&quot;&gt;View PDF&lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;pdf-embed-container-dee20d12-d409-41d3-9075-6a820e31caab&quot;&gt; &lt;iframe src=&quot;/pdfs/LCT_Annual_Meeting_2023.pdf&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;/div&gt;</content><author><name></name></author><category term="blog" /><category term="ml" /><summary type="html">This poster was presented at the Annual LCT Meeting, University of Malta, June 2023 and is based on the work by Wang et al. Large Language Models(LLM’s) store linguistic and relational information. The knowledge gained by LLM’s can be exploited and extracted to perform partially unsupervised open information extraction. Extracting n-ary representations from text in an unsupervised manner helps in many tasks, such as building knowledge bases and knowledge graph construction.</summary></entry><entry><title type="html">On Cross Lingual Learning and Dravidian Languages</title><link href="http://localhost:4000/blog/ml/2023/04/25/cll-dravidian_languages.html" rel="alternate" type="text/html" title="On Cross Lingual Learning and Dravidian Languages" /><published>2023-04-25T12:17:24+02:00</published><updated>2023-04-25T12:17:24+02:00</updated><id>http://localhost:4000/blog/ml/2023/04/25/cll-dravidian_languages</id><content type="html" xml:base="http://localhost:4000/blog/ml/2023/04/25/cll-dravidian_languages.html">&lt;p&gt;Despite their richness and diversity, low-resource languages have not received as much attention from NLP researchers as high-resource languages like English and Spanish. However, recent progress in transfer learning, unsupervised learning, and data augmentation techniques show promise for improving NLP systems for low-resource languages. Leveraging the latent symmetry learned by multilingual language models through joint training, this report explores how cross-lingual learning can benefit the understanding of Dravidian languages, specifically, Telugu, Malayalam and Tamil. The blogpost covers tasks related to question answering, transliteration, code-switching, and hate speech detection. This non-exhaustive survey aims to facilitate further research in these important and socially beneficial tasks.&lt;/p&gt;

&lt;!--more--&gt;

&lt;style&gt; .pdf-embed-wrap-cbfdf60a-366d-4721-878c-3f7fd8fc484d { display: flex; flex-direction: column; width: 100%; height: 650px; } .pdf-embed-container-cbfdf60a-366d-4721-878c-3f7fd8fc484d { height: 100%; } .pdf-link-cbfdf60a-366d-4721-878c-3f7fd8fc484d { background-color: white; text-align: center; border-style: solid; } .pdf-embed-container-cbfdf60a-366d-4721-878c-3f7fd8fc484d iframe { width: 100%; height: 100%; } &lt;/style&gt;
&lt;div class=&quot;pdf-embed-wrap-cbfdf60a-366d-4721-878c-3f7fd8fc484d&quot;&gt; &lt;div class=&quot;pdf-link-cbfdf60a-366d-4721-878c-3f7fd8fc484d&quot;&gt; &lt;a href=&quot;/pdfs/cll_dravidian_langs.pdf&quot; target=&quot;_blank&quot;&gt;View PDF&lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;pdf-embed-container-cbfdf60a-366d-4721-878c-3f7fd8fc484d&quot;&gt; &lt;iframe src=&quot;/pdfs/cll_dravidian_langs.pdf&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;/div&gt;</content><author><name></name></author><category term="blog" /><category term="ml" /><summary type="html">Despite their richness and diversity, low-resource languages have not received as much attention from NLP researchers as high-resource languages like English and Spanish. However, recent progress in transfer learning, unsupervised learning, and data augmentation techniques show promise for improving NLP systems for low-resource languages. Leveraging the latent symmetry learned by multilingual language models through joint training, this report explores how cross-lingual learning can benefit the understanding of Dravidian languages, specifically, Telugu, Malayalam and Tamil. The blogpost covers tasks related to question answering, transliteration, code-switching, and hate speech detection. This non-exhaustive survey aims to facilitate further research in these important and socially beneficial tasks.</summary></entry><entry><title type="html">On Dual Encoders</title><link href="http://localhost:4000/blog/ml/2023/04/12/dual-encoders.html" rel="alternate" type="text/html" title="On Dual Encoders" /><published>2023-04-12T20:45:24+02:00</published><updated>2023-04-12T20:45:24+02:00</updated><id>http://localhost:4000/blog/ml/2023/04/12/dual-encoders</id><content type="html" xml:base="http://localhost:4000/blog/ml/2023/04/12/dual-encoders.html">&lt;p&gt;This blogpost attempts to provide a glimpse at the potential and different applications of dual encoders while keeping in mind their pitfalls too and is inspired by Luan et al. (2021) and their use of dual encoders as first stage retrievers.&lt;/p&gt;

&lt;!--more--&gt;

&lt;style&gt; .pdf-embed-wrap-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c { display: flex; flex-direction: column; width: 100%; height: 650px; } .pdf-embed-container-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c { height: 100%; } .pdf-link-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c { background-color: white; text-align: center; border-style: solid; } .pdf-embed-container-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c iframe { width: 100%; height: 100%; } &lt;/style&gt;
&lt;div class=&quot;pdf-embed-wrap-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c&quot;&gt; &lt;div class=&quot;pdf-link-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c&quot;&gt; &lt;a href=&quot;/pdfs/dual_encoders.pdf&quot; target=&quot;_blank&quot;&gt;View PDF&lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;pdf-embed-container-e0982263-d5c3-4c6c-b0fa-7c37f3cd076c&quot;&gt; &lt;iframe src=&quot;/pdfs/dual_encoders.pdf&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;/div&gt;</content><author><name></name></author><category term="blog" /><category term="ml" /><summary type="html">This blogpost attempts to provide a glimpse at the potential and different applications of dual encoders while keeping in mind their pitfalls too and is inspired by Luan et al. (2021) and their use of dual encoders as first stage retrievers.</summary></entry><entry><title type="html">On Pruning Large Language Model’s</title><link href="http://localhost:4000/blog/ml/2023/04/01/pruning-dnn.html" rel="alternate" type="text/html" title="On Pruning Large Language Model’s" /><published>2023-04-01T20:43:24+02:00</published><updated>2023-04-01T20:43:24+02:00</updated><id>http://localhost:4000/blog/ml/2023/04/01/pruning-dnn</id><content type="html" xml:base="http://localhost:4000/blog/ml/2023/04/01/pruning-dnn.html">&lt;p&gt;This blogpost describe the compressing of BERT, in the context of the Lottery Ticket Hypothesis. Through emperical evidence obtained by fine tuning on several tasks, it is found that &lt;em&gt;30-40%&lt;/em&gt; of the parameters in the BERT model can be discarded.&lt;/p&gt;

&lt;!--more--&gt;

&lt;style&gt; .pdf-embed-wrap-ea9e86c4-842c-4421-a6e1-111bce50bc72 { display: flex; flex-direction: column; width: 100%; height: 650px; } .pdf-embed-container-ea9e86c4-842c-4421-a6e1-111bce50bc72 { height: 100%; } .pdf-link-ea9e86c4-842c-4421-a6e1-111bce50bc72 { background-color: white; text-align: center; border-style: solid; } .pdf-embed-container-ea9e86c4-842c-4421-a6e1-111bce50bc72 iframe { width: 100%; height: 100%; } &lt;/style&gt;
&lt;div class=&quot;pdf-embed-wrap-ea9e86c4-842c-4421-a6e1-111bce50bc72&quot;&gt; &lt;div class=&quot;pdf-link-ea9e86c4-842c-4421-a6e1-111bce50bc72&quot;&gt; &lt;a href=&quot;/pdfs/pruning_DNN.pdf&quot; target=&quot;_blank&quot;&gt;View PDF&lt;/a&gt; &lt;/div&gt; &lt;div class=&quot;pdf-embed-container-ea9e86c4-842c-4421-a6e1-111bce50bc72&quot;&gt; &lt;iframe src=&quot;/pdfs/pruning_DNN.pdf&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt; &lt;/div&gt; &lt;/div&gt;</content><author><name></name></author><category term="blog" /><category term="ml" /><summary type="html">This blogpost describe the compressing of BERT, in the context of the Lottery Ticket Hypothesis. Through emperical evidence obtained by fine tuning on several tasks, it is found that 30-40% of the parameters in the BERT model can be discarded.</summary></entry><entry><title type="html">A look at Diglossia</title><link href="http://localhost:4000/blog/ml/2022/07/22/diglossia.html" rel="alternate" type="text/html" title="A look at Diglossia" /><published>2022-07-22T00:13:24+02:00</published><updated>2022-07-22T00:13:24+02:00</updated><id>http://localhost:4000/blog/ml/2022/07/22/diglossia</id><content type="html" xml:base="http://localhost:4000/blog/ml/2022/07/22/diglossia.html">&lt;p&gt;The term Diglossia, derived from the Greek word, &lt;em&gt;diglōssos&lt;/em&gt;(two tongues) was brought into popular usage in the English language by Charles Ferguson who wrote an influential paper of the same name (Ferguson, 1959).&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Diglossia is loosely defined as a situation where two varieties of a language exist in a speech community, each with its specific functions.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;In fact, &lt;em&gt;diglossia can be extended to include not just varieties of a language, but two separate languages too.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;The two linguistic varieties are denoted by “H” and “L”. When this system was introduced in the 1950s, “H” represented “high” and “L” represented “low”. This was to designate the “high” category as the relatively more respected variety, used for official purposes like speeches by political leaders, newspapers, journals etc, while “low” is the variety used within the community and informally between family and friends. Currently, the convention is to simply use H and L without the connotation of one being superior to another (Pym, 2019).&lt;/p&gt;

&lt;p&gt;Before progressing any further, mentioning a standard definition of diglossia would be prudent. As defined by Ferguson,&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Diglossia is a relatively stable language situation in which, in addition to the primary dialects of the language (which may include a standard or regional standards), there is a
very divergent, highly codified (often grammatically more complex) superposed variety, the vehicle of a large and respected body of written literature, either of an earlier period or in another speech community, which is learned largely by formal education and is used for most written and formal spoken purposes but is not used by any sector of the community for ordinary conversation. (Ferguson, 1959, p. 1)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p align=&quot;center&quot;&gt;
  &lt;img width=&quot;460&quot; height=&quot;300&quot; src=&quot;/assets/images/diglossia.jpeg&quot; alt=&quot; source is https://alphaomegatranslations.com/foreign-language/the-phenomenon-of-diglossia/&quot; /&gt;
  &lt;center&gt;&lt;p&gt;&lt;em&gt;Diglossia in Greece : Tsakonian language&lt;/em&gt;&lt;/p&gt;&lt;/center&gt;
&lt;/p&gt;

&lt;p&gt;Deconstructing the above definition, the phrase “relatively stable language situation” can be interpreted as stability in terms of the existence of the diglossic situation. For example, some languages like Arabic have existed in a diglossic situation as long as the language has existed while for other languages like Swiss German, diglossia developed as a result of political changes and isolation from the centres of German linguistic standardization (Ferguson, 1959, p. 4).&lt;/p&gt;

&lt;p&gt;Looking at stability as a function of whether a language variety stays H or L through the centuries however would not work since a variety of romance languages started as L languages, in a far less superior position compared to the H languages of the time (for example, Latin) and in the present time, these languages are generally in the H position (Pym, 2019).&lt;/p&gt;

&lt;p&gt;The definition mentions that &lt;em&gt;the superposed variety(H) is “highly codified”&lt;/em&gt;, that is, there is a large body of literature concerning the correct usage of the superposed variety. This is contradicted by Kaye (1972) in his paper “Remarks on diglossia in Arabic: defined vs. ill-defined”. In the case of Arabic, MSA(Modern Standard Arabic) is considered H and the L can be any one of the varieties spoken in the Middle East.&lt;/p&gt;

&lt;p&gt;According to Kaye, it is easier for a linguist to define what MSA isn’t rather than defining what it is and the phonology of MSA cannot be described with the same ease as that of the L of Arabic. (Kaye, 1972, p. 2). In my opinion, for a lot of cases, the H form is indeed better described than the L form, an example would be the diglossia between Hindi(H) and Hindustani(L). Most grammar books explain and teach Hindi which has a different vocabulary compared to Hindustani which is spoken informally and has plenty of loan words from Urdu.&lt;/p&gt;

&lt;p&gt;The definition also mentions “very divergent”. This can be interpreted in the sense that&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;the differences between the “canonic” cases of diglossia, i.e, the 4 diglossic situations mentioned in the paper(Swiss German, Haitian Creole, Modern Arabic and Greek) and their H counterparts cannot be explained simply by the environment of the speakers.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The differences go down to orthography, inflectional morphology and phonology. For example, in Cairene, /beet/ (house) corresponds to &lt;em&gt;/baytu/&lt;/em&gt;(nominative), &lt;em&gt;/bayta/&lt;/em&gt;(accusative) and &lt;em&gt;/bayti/&lt;/em&gt;(genitive) which means that categories of case are unrecognized for Cairene. (Kaye, 1972, p. 41)&lt;/p&gt;

&lt;p&gt;Ferguson’s initial paper on diglossia focussed on a single language being characterized by the diglossic situation while some later interpretations have expanded the definition of diglossia to include situations in a speech community where two different languages are used in the H and L functions. (Gumperz, 1971)(Hawkins, 1983, p. 14).&lt;/p&gt;

&lt;p&gt;Such a situation is observed in Paraguay where almost everyone can speak both Spanish and Guarani, Spanish being the H and Guarani being the L. Hence, diglossia is not primarily a monolingual situation but can exist in bilingual and multilingual situations (Fishman, 1967).&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Arguments have been made against the stance of expanding the definition of diglossia to include multidialectal situations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Multidialectal/Multilingual contexts include the diglossic situation between Haitian Creole and French in Haiti. “True” diglossic situations like the one in Greek or Arabic are different from multilingual situations in the sense that the L for both these languages is natively spoken and it varies from place to place. Descriptive grammar can be written for the L variety.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;The H variety in this scenario is obtained by “purification” rules. In the multilingual situation, the direct opposite is done, where the H exists as a natively spoken, independent variety and the L is derived from the standard H. The former is the bottom-up approach while the latter is the top-down approach.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The stance, in this argument, is that the definition of diglossia should be narrowed, instead of expanded to fit creole-like/multi-dialectal situations (Hawkins, 1983, p. 1)&lt;/p&gt;

&lt;p&gt;Further, diglossia in each speech community can be characterized by the eight features mentioned in Ferguson(1959). The features as detailed by Ferguson are described below along with the contradictions and commentary by various authors over the decades that have passed.&lt;/p&gt;

&lt;h3 id=&quot;features-of-diglossia&quot;&gt;Features of Diglossia&lt;/h3&gt;

&lt;h4 id=&quot;function&quot;&gt;&lt;em&gt;Function&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;According to Ferguson, each variety in the diglossic situation has a specific scenario in which it might be used.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Using L in a public speech would be inappropriate, similar to using H in informal situations.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;An extension to this definition (especially for diglossic situations where the diglossia is between two different languages) would be that a language that occupies the L position may also be H at the same time in a different setting.&lt;/p&gt;

&lt;p&gt;An example would be the situation in Catalonia where a few decades ago, Catalan would be the L language and Spanish the H language. Due to the standardization of Catalan and the push towards making it more dominant, it has assumed the position of the H language, especially in Barcelona. For the immigrant groups, Spanish can be the L language and again at the same time, for the rural folk, a different variation of Catalan is the L 
variety (Pym, 2019).&lt;/p&gt;

&lt;h4 id=&quot;prestige&quot;&gt;&lt;em&gt;Prestige&lt;/em&gt;&lt;/h4&gt;
&lt;blockquote&gt;
  &lt;p&gt;H is associated with higher prestige and L is considered to be the language of the “uneducated” and the “illiterate”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;H is considered to be beautiful, perfect and the standard to aspire to. In certain cases, the superiority of H over L is due to religious reasons, examples would be the case of Greek and of Arabic.&lt;/p&gt;

&lt;p&gt;Ferguson suggests that the feeling of H being superior compared to L is so strong that L is reported to “not exist” and this is not due to the speakers of the language deliberately trying to mislead the authors. Frequently, with regards to the diglossic situation in Arabic, a fluent speaker of L who does not know H will be reported as someone who does not have knowledge of Arabic.&lt;/p&gt;

&lt;h4 id=&quot;literary-heritage&quot;&gt;&lt;em&gt;Literary Heritage&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;There is a sizable amount of literature written in H, that is, H is highly codified and regularly the literature in H is considered to be more prestigious. But, literary works are not necessarily confined to H but can be a combination of both H and L or could be just L too.&lt;/p&gt;

&lt;p&gt;In Greek, an example of a purely H text would be P. KaUigha’s &lt;em&gt;‘Thanos Vlekas’&lt;/em&gt;(1855), purely L text is &lt;em&gt;‘Plusii ke Ftochi’&lt;/em&gt; by G. Xenopoulos(1907) and a mixture of the two is A. Papadiamantis, &lt;em&gt;‘Stringla Manna’&lt;/em&gt; (1905) (Hawkins, 1983, p. 5).&lt;/p&gt;

&lt;h4 id=&quot;acquisition&quot;&gt;&lt;em&gt;Acquisition&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;The knowledge of L is acquired at home, with friends and family while H is learnt in school. In the Arab world, this is especially a problem since the language spoken at home(usually Cairene) differs from the one taught in school which is MSA (SAIEGH–HADDAD, E., 2003).&lt;/p&gt;

&lt;h4 id=&quot;standardization&quot;&gt;&lt;em&gt;Standardization&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;Ferguson states that H is more codified and there are grammars, dictionaries and so on that exist for H while for L, the documentation is conducted by scholars outside the community. This view is challenged by Kaye(1972) who presents two models of diglossia, the process model and the static model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The process model considers L as the standard and derives H from it using a series of “purification” rules while the static model considers the H and L as two “poles of reference”.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;They argue that in Arabic, it is the various L’s that are stable while the H is “ill-defined”. How an individual uses H depends on their social background or their level of education. Hence, Kaye sides with the process model of diglossia that they theorized while also elaborating on the positives of the static model in their paper.&lt;/p&gt;

&lt;h4 id=&quot;stability&quot;&gt;&lt;em&gt;Stability&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;As mentioned above while discussing the definition, diglossic situations can arise and exist alongside the language and this has been the case for Arabic while Haitian Creole arose from the creolization of pidgin French. Greek diglossia has existed for centuries but it achieved its present form only in the nineteenth century (Ferguson, 1959, p. 4).&lt;/p&gt;

&lt;h4 id=&quot;grammar&quot;&gt;&lt;em&gt;Grammar&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;A common consensus in the writings on diglossia seems to be that the grammatical structures of the H and L variety are quite different. MSA nouns have three cases, most colloquial dialects have none. Cairene, a colloquial dialect, is simpler than MSA. There is a syntactic difference as well between the two, where Cairene uses “of” and MSA does not consider such a particle or syntactic construction. Haitian Creole, one of the defining languages in Ferguson(1959) does not use the gender or number in the noun while French, the H counterpart of Haitian Creole uses it (Ferguson, 1959, p. 10). Another unanimous opinion is that typically, L has a “simpler” grammar compared to H.&lt;/p&gt;

&lt;h4 id=&quot;lexicon&quot;&gt;&lt;em&gt;Lexicon&lt;/em&gt;&lt;/h4&gt;
&lt;p&gt;H and L generally share quite a lot of words. Words that exist in H might not exist in L because topics that are raised while speaking H would by and large not be spoken in L. A curious situation that is pointed out in Ferguson(1959) is that certain terms have words in both H and L and using either word immediately indicates if a person is speaking in H or L.&lt;/p&gt;

&lt;p&gt;From personal experience, the word for “book” in Hindustani(L) is &lt;em&gt;kitaab&lt;/em&gt; which is presumably a loan word from Urdu, while in Hindi(H) the word is &lt;em&gt;pustak&lt;/em&gt;, which is a word derived from Sanskrit. Similarly, in Katharevousa, the H form of Greek, the word for wine is &lt;em&gt;ínos&lt;/em&gt; and in L it is &lt;em&gt;krasí&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Often Diglossia and bilingualism are interconnected such that diglossia can exist with bilingualism and without it too. A case of both diglossia and bilingualism would be in Paraguay as mentioned above where almost the entire country speaks in Spanish(H) and Guarani(L). According to Fishman(1967), most examples of such a situation where both diglossia and bilingualism exist are because the community has available to them “compartmentalized roles” (Fishman, 1967, p. 5).&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h3&gt;

&lt;p&gt;Diglossia is a vast topic and one blanket definition will not be enough to cover all cases of this situation. Often there is an overlap between diglossia and bilingualism but a clear distinction can be drawn here since bilingualism is on an individual level and &lt;em&gt;diglossia is a social situation: it affects societies&lt;/em&gt; (Pym, 2019).&lt;/p&gt;

&lt;p&gt;Despite various authors postulating their own definitions of diglossia based on the different linguistic situations, it is difficult to find a comprehensive definition of diglossia that applies to all languages. In my opinion, the definition of diglossia should be specific to speech communities/languages. A loose definition of diglossia may be presented here.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Diglossia is a social situation where two language varieties are used under different conditions by a speech community.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This definition covers the basic requirement that a diglossic situation comes under.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Ferguson, C. A. (1959). Diglossia. word, 15(2), 325-340&lt;/li&gt;
  &lt;li&gt;Pym, Anthony, “Diglossia” (2019). Open Educational Resources Collection. 16&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Language in social groups JJ Gumperz - 1971 - Stanford University Press Stanford&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Hawkins, P. (1983). Diglossia revisited. Language Sciences, 5(1), 1–20.&lt;/li&gt;
  &lt;li&gt;Kaye, A. S. (1972). Remarks on diglossia in Arabic: well-defined vs. ill-defined.&lt;/li&gt;
  &lt;li&gt;SAIEGH–HADDAD, E. L. I. N. O. R. (2003). Linguistic distance and initial reading acquisition: The case of Arabic diglossia. Applied Psycholinguistics, 24(3), 431-451.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Fishman, J. A. (1986). Introduction to sociolinguistics, I: The sociolinguistics of society By Ralph Fasold. Language, 62(1), 188-189.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Fishman, J. A. (1967). Bilingualism with and without diglossia; diglossia with and without bilingualism. Journal of social issues, 23(2), 29-38.&lt;/li&gt;
&lt;/ul&gt;</content><author><name></name></author><category term="blog" /><category term="ml" /><summary type="html">The term Diglossia, derived from the Greek word, diglōssos(two tongues) was brought into popular usage in the English language by Charles Ferguson who wrote an influential paper of the same name (Ferguson, 1959).</summary></entry><entry><title type="html">About Interpretable Machine Learning</title><link href="http://localhost:4000/blog/ml/2021/06/24/interpretable-ai.html" rel="alternate" type="text/html" title="About Interpretable Machine Learning" /><published>2021-06-24T20:43:24+02:00</published><updated>2021-06-24T20:43:24+02:00</updated><id>http://localhost:4000/blog/ml/2021/06/24/interpretable-ai</id><content type="html" xml:base="http://localhost:4000/blog/ml/2021/06/24/interpretable-ai.html">&lt;p&gt;Interpretable machine learning, as specified by Carvalho, Diogo V et al (2019) in “Machine learning interpretability: A survey on methods and metrics” enables a user to verify, interpret and understand the reasoning of a system.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Another succinct definition describes the properties of such a model.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;An interpretable model is one where a human can contemplate it at once (simulatability), based on a full understanding of the algorithm (algorithmic transparency) where each input is by itself interpretable (decomposability) &lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;A learned model is an implicit, dynamically varying relationship that learns from the data provided to it, and is constantly updated as new information is added as input. Explaining what they predict, otherwise known as interpretable machine learning, is a relatively small subset of research when compared to the focus on achieving better performance metrics for these models &lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;Interest in the area of machine learning interpretability has seen resurgence &lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; since many real-world problems in different domains are being tackled using machine learning and artificial intelligence. Sensitive issues like recidivism prediction, detection of money-laundering in financial systems, and healthcare are being addressed using increasingly complex models.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;In delicate scenarios, being certain of the path the model takes to arrive at a decision is necessary to rule out algorithmic bias due to training data.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The need for interpretability is also due to incompleteness in the problem statement as elaborated by Doshi-Velez and Kim(2017)&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. Some situations such as movie recommender systems do not require an explanation due to the simple fact that the output does not have a significant effect on a human, while a recidivism prediction system should justify its conclusion to rule out any bias, say, on racial grounds. The problem statement here would not only involve predictions, but also an explanation of the reason behind such a prediction due to its impact on human life.&lt;/p&gt;

&lt;p&gt;Building on this, the “European Union Regulations on Algorithmic Decision Making and A Right to Explanation” by Bryce Goodman and Seth Flaxman, dictates a “right to explanation”. Hence, interpretable models seem to be the need of the hour.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Considering the widespread penetration of machine learning in day-to-day life, a natural question arises: Can you trust the model?&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;This can be perceived on two levels, trusting a model and trusting an individual prediction. One prerequisite to trust, whether towards a model or a prediction is understanding/knowledge of its behaviour, which is where techniques such as LIME (Local Interpretable Model-Agnostic Explanation) come in handy &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;. As elaborated in the cited paper, LIME attempts to explain a prediction by presenting a simple model in the neighbourhood of the instance in question.&lt;/p&gt;

&lt;p&gt;An interesting example provided in this paper takes the case of a text classifier that uses SVM’s on a subset of the 20 group dataset to differentiate ‘Atheism’ from ‘Christianity’. The accuracy rate was very high, but the LIME technique reported that the decision was made for random reasons like the occurrence of words that have no connection with either classification. It is tempting to use a model with such a high accuracy until we discover the reason behind the model’s decisions. Hence, high accuracy on the test set does not always assure that a model will perform well.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Techniques such as LIME can be used together with these models to assess the prediction and encourage trust among users.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Model audits are another way to ensure trust since audits give a sense of the reliability of the model. One such example is the Resampling Uncertainty Estimation (RUE), an algorithm that addresses reliability by creating “an ensemble of predictions” and “estimates the amount that a prediction would change if the model had been fit on different training data” &lt;sup id=&quot;fnref:6&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:6&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;6&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;RUE is one among many algorithms that seek to audit models, a prominent feature is the fact that it audits after learning and provides an uncertainty score for each prediction.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;So far, this essay has discussed trust based on an understanding of the inner workings of algorithms and on the basis of a framework that would measure reliability through uncertainty scores calculated during the testing phase. At this point, taking a step back to compare the decision-making skills of a model against a human’s thought process can also be treated as a component that goes into trusting it. Is the user of the model comfortable in giving up control of the situation? Would a human truly perform better in cases that involve racial and gender bias? In a situation like this, a model may come to a conclusion similar to the one that a human would arrive at, simply because the dataset may reflect a human’s prejudice.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Another aspect of trust would mean that we can rely on a model’s decisions if a human could make the same error.&lt;sup id=&quot;fnref:2:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;We can conclude this discussion with an important point. Trusting a model depends largely on its use case. When it is a low-stakes situation such as a product recommendation system, an incorrect prediction would at best waste a few minutes of a user’s time. In higher impact situations, it seems very risky to have blind faith in the model due to issues such as dataset shift &lt;sup id=&quot;fnref:7&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:7&quot; class=&quot;footnote&quot; rel=&quot;footnote&quot;&gt;7&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;

&lt;p&gt;Again, while model interpretability techniques do provide an insight into how models work, they also fulfil human curiosity. Since the beginning of time, humans have tried to deepen their understanding of the universe and it is natural to have questions about the black-box implementation of some models. Understanding the reasoning behind a decision will also lead to greater societal acceptance towards machine learning and artificial intelligence.&lt;/p&gt;

&lt;p&gt;All in all, any advance in explainable machine learning models would lead to the capabilities of AI being leveraged in every field, as people would encourage the use of transparent systems. Further, this would accelerate the research in the field of data science as a whole.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Lipton, Z. C. (2018). The Mythos of Model Interpretability: In machine learning, the concept of interpretability is both important and slippery. Queue, 16(3), 31-57. &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt; &lt;a href=&quot;#fnref:2:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Carvalho, D. V., Pereira, E. M., &amp;amp; Cardoso, J. S. (2019). Machine learning interpretability: A survey on methods and metrics. Electronics, 8(8), 832. &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Linardatos, P., Papastefanopoulos, V., &amp;amp; Kotsiantis, S. (2021). Explainable AI: A Review of Machine Learning Interpretability Methods. Entropy, 23(1), 18. &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Doshi-Velez, F., &amp;amp; Kim, B. (2017). Towards a rigorous science of interpretable machine learning. arXiv preprint arXiv:1702.08608. &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Ribeiro, M. T., Singh, S., &amp;amp; Guestrin, C. (2016, August). “ Why should i trust you?” Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD international conference on knowledge discovery and data mining (pp. 1135-1144). &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:6&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Schulam, P., &amp;amp; Saria, S. (2019, April). Can you trust this prediction? Auditing pointwise reliability after learning. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 1022-1031). PMLR. &lt;a href=&quot;#fnref:6&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:7&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;Moreno-Torres, J. G., Raeder, T., Alaiz-Rodríguez, R., Chawla, N. V., &amp;amp; Herrera, F. (2012). A unifying view on dataset shift in classification. Pattern recognition, 45(1), 521-530. &lt;a href=&quot;#fnref:7&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name></name></author><category term="blog" /><category term="ml" /><summary type="html">Interpretable machine learning, as specified by Carvalho, Diogo V et al (2019) in “Machine learning interpretability: A survey on methods and metrics” enables a user to verify, interpret and understand the reasoning of a system.</summary></entry><entry><title type="html">Barebones Search Engine Implementation</title><link href="http://localhost:4000/projects/2020/08/29/search-engine.html" rel="alternate" type="text/html" title="Barebones Search Engine Implementation" /><published>2020-08-29T20:43:24+02:00</published><updated>2020-08-29T20:43:24+02:00</updated><id>http://localhost:4000/projects/2020/08/29/search-engine</id><content type="html" xml:base="http://localhost:4000/projects/2020/08/29/search-engine.html">&lt;p&gt;This blog documents the process of implementing a search engine that takes a query term as input and ranks relevant documents in the corpus.&lt;/p&gt;

&lt;!--more--&gt;

&lt;p&gt;The corpus, in this case is a collection of random documents, that is, wiki’s of a tv show’s prominent characters, a movie franchise, a country, a social media company and a type of fish.&lt;/p&gt;

&lt;p&gt;Code at : &lt;a href=&quot;https://github.com/amrtanair/search-engine&quot;&gt;amrtanair/search-engine&lt;/a&gt;&lt;/p&gt;

&lt;h3 id=&quot;breaking-down-the-problem-and-the-approach&quot;&gt;Breaking down the problem and the approach&lt;/h3&gt;

&lt;p&gt;The first step was to create the inverted index(a dictionary that stores the mapping of a term to its occurrences in a document/corpus) for a single document. This involved using the requests library to fetch the contents of a webpage and the BeautifulSoup library to parse and store the text content on the webpage. The resulting textfile was then stored as tokens(a list that contains all words in the document(minus the stopwords) in lowercase format) and a document list. Later, the inverted index was created using the python dictionary data type.&lt;/p&gt;

&lt;p&gt;The second step was doing the same on the corpus, that is, creating an inverted index that included the position of every relevant term in every document. The way to do this was to create a &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mother_inverted_index&lt;/code&gt; that would be a collection of all inverted indexes such that the value for every key in the dictionary is a list of lists, something like this: 
Let’s say the word “harry”, appears in documents 1, 3, but not 2. The record in the hashmap would be as follows:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;'harry' =&amp;gt; [[1, [45, 67, 54]], [2, [2, 5, 4]]]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Every time the inverted index for a doc was created, it was ‘merged’ into the mother inverted index by looping through every term in the newly created one: adding the term to the larger inverted index if it doesn’t exist and if it does exist, add it to the existing key/value pair in the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;mother_inverted_index&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;The next step was the create a basic search engine, sans the ranking. A string was taken as an input and each word was compared with the inverted index.&lt;/p&gt;

&lt;h3 id=&quot;ranking&quot;&gt;Ranking&lt;/h3&gt;

&lt;p&gt;&lt;a href=&quot;https://en.wikipedia.org/wiki/Tf%E2%80%93idf&quot;&gt;tf-idf&lt;/a&gt; was used to rank the relevant documents. It describes how ‘important’ a term is in a given document. There are different versions of implementing this algorithm. Tf-idf here will be calculated using the method &lt;a href=&quot;https://github.com/scikit-learn/scikit-learn/blob/0fb307bf3/sklearn/feature_extraction/text.py#L1322&quot;&gt;sci-kit&lt;/a&gt; uses in the hyperlinked file.&lt;/p&gt;

&lt;p&gt;After the tf-idf was calculated, the vectors for each document was also normalised using the l2-norm normalization technique. Normalisation was performed because we would be comparing each document’s vector with a query vector. This query vector is also k-dimensional and normalised.&lt;/p&gt;

&lt;p&gt;The similarity between the query vector and each relevant document’s vector was done using the cosine similarity method which calculates the angle between two vectors in a multi-dimensional space. Smaller the angle, the higher the similarity. 
The value returned is a list of lists of all document URLs with the similarity index in such a format:&lt;/p&gt;
&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[[url1, cosine1], [url2, cosine2]]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;h3 id=&quot;work-ahead&quot;&gt;Work ahead&lt;/h3&gt;

&lt;p&gt;There are some points where this implementation can be improved:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Stemming the tokens in the corpus and the query. Different forms of a word can be searched for it would make the resulting inverted index cleaner.&lt;/li&gt;
  &lt;li&gt;A cleaner way of generating tfidf. There is room for improvement, using a different data structure or routine would make it more space/time efficient.&lt;/li&gt;
  &lt;li&gt;Dividing the modules into classes better and using OOP methodology to structure this.&lt;/li&gt;
  &lt;li&gt;Using a much more diverse dataset.&lt;/li&gt;
&lt;/ol&gt;

&lt;blockquote&gt;
  &lt;p&gt;Note: I am still learning python and I have a long long way to go. I will be updating this blog and the Github repository as I get better at writing pythonic code. This is still a WIP :)&lt;/p&gt;
&lt;/blockquote&gt;</content><author><name></name></author><category term="projects" /><summary type="html">This blog documents the process of implementing a search engine that takes a query term as input and ranks relevant documents in the corpus.</summary></entry></feed>